## 1-Requirements quality research a harmonized theory, evaluation


Frattini, J., Montgomery, L., Fischbach, J. et al. Requirements quality research: a harmonized theory, evaluation, and roadmap. Requirements Eng 28, 507–520 (2023). https://doi.org/10.1007/s00766-023-00405-y



## 2-Requirements and software engineering for automotive perception


Habibullah, K.M., Heyn, HM., Gay, G. et al. Requirements and software engineering for automotive perception systems: an interview study. Requirements Eng (2024). https://doi.org/10.1007/s00766-023-00410-1



## 3-An empirical study on software understandability


Lavazza, L., Morasca, S. & Gatto, M. An empirical study on software understandability and its dependence on code characteristics. Empir Software Eng 28, 155 (2023). https://doi.org/10.1007/s10664-023-10396-7



## 4-InspectJS Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript


@misc{dutta2021inspectjs,

title={InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript},

author={Saikat Dutta and Diego Garbervetsky and Shuvendu Lahiri and Max Schäfer},

year={2021},

eprint={2111.09625},

archivePrefix={arXiv},

primaryClass={cs.CR}

}




## 5-Competencies for Code Review


@article{10.1145/3579471,

author = {Wurzel Gon\c{c}alves, Pavl\'{\i}na and Calikli, G\"{u}l and Serebrenik, Alexander and Bacchelli, Alberto},

title = {Competencies for Code Review},

year = {2023},

issue\_date = {April 2023},

publisher = {Association for Computing Machinery},

address = {New York, NY, USA},

volume = {7},

number = {CSCW1},

url = {https://doi.org/10.1145/3579471},

doi = {10.1145/3579471},

abstract = {Peer code review is a widely practiced software engineering process in which software developers collaboratively evaluate and improve source code quality. Whether developers can perform good reviews depends on whether they have sufficient competence and experience. However, the knowledge of what competencies developers need to execute code review is currently limited, thus hindering, for example, the creation of effective support tools and training strategies. To address this gap, we firstly identified 27 competencies relevant to performing code review through expert validation. Later, we conducted an online survey with 105 reviewers to rank these competencies along four dimensions: frequency of usage, importance, proficiency, and desire of reviewers to improve in that competency. The survey shows that technical competencies are considered essential to performing reviews and that respondents feel generally confident in their technical proficiency. Moreover, reviewers feel less confident in how to communicate clearly and give constructive feedback - competencies they consider like-wise an essential part of reviewing. Therefore, research and education should focus in more detail on how to support and develop reviewers' potential to communicate effectively during reviews. In the paper, we also discuss further implications for training, code review performance assessment, and reviewers of different experience level.Data and materials: https://doi.org/10.5281/zenodo.7401313},

journal = {Proc. ACM Hum.-Comput. Interact.},

month = {apr},

articleno = {38},

numpages = {33},

keywords = {code review, competency, human factors, skills, training}

}


## 6-Does UML Modeling Associate with Lower Defect Proneness?: A Preliminary Empirical Investigation


@INPROCEEDINGS{8816762,

author={Raghuraman, Adithya and Ho-Quang, Truong and Chaudron, Michel R. V. and Serebrenik, Alexander and Vasilescu, Bogdan},

booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},

title={Does UML Modeling Associate with Lower Defect Proneness?: A Preliminary Empirical Investigation},

year={2019},

volume={},

number={},

pages={101-104},

keywords={Unified modeling language;Computer bugs;Data models;Open source software;Data mining;Java;software design, UML, software quality, open-source-software},

doi={10.1109/MSR.2019.00024}}


## 7-The\_Silent\_Helper\_The\_Impact\_of\_Continuous\_Integration\_on\_Code\_Reviews


@INPROCEEDINGS{9054818,

author={Cassee, Nathan and Vasilescu, Bogdan and Serebrenik, Alexander},

booktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},

title={The Silent Helper: The Impact of Continuous Integration on Code Reviews},

year={2020},

volume={},

number={},

pages={423-434},

keywords={Codes;Conferences;Focusing;Software quality;Data models;Open source software;Software engineering},

doi={10.1109/SANER48275.2020.9054818}}


## 8-Towards\_Automating\_Code\_Review\_Activities


@inproceedings{10.1109/ICSE43902.2021.00027,

author = {Tufano, Rosalia and Pascarella, Luca and Tufano, Michele and Poshyvanyk, Denys and Bavota, Gabriele},

title = {Towards Automating Code Review Activities},

year = {2021},

isbn = {9781450390859},

publisher = {IEEE Press},

url = {https://doi.org/10.1109/ICSE43902.2021.00027},

doi = {10.1109/ICSE43902.2021.00027},

abstract = {Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code.Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the contributor and the reviewer sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language.The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16\% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31\% of cases. While these results are encouraging, more research is needed to make these models usable by developers.},

booktitle = {Proceedings of the 43rd International Conference on Software Engineering},

pages = {163–174},

numpages = {12},

keywords = {Code Review, Deep Learning, Empirical Software Engineering},

location = {Madrid, Spain},

series = {ICSE '21}

}

Tufano, Rosalia; Pascarella, Luca; Tufano, Michele; Poshyvanyk, Denys; Bavota, Gabriele. "Towards Automating Code Review Activities," in the Proceedings of the 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 2021. doi: 10.1109/ICSE43902.2021.00027

## 9-An\_Empirical\_Evaluation\_of\_GitHub\_Copilots\_Code\_Suggestions


@INPROCEEDINGS{9796235,

author={Nguyen, Nhan and Nadi, Sarah},

booktitle={2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)},

title={An Empirical Evaluation of GitHub Copilot's Code Suggestions},

year={2022},

volume={},

number={},

pages={1-5},

keywords={Measurement;Java;Codes;Static analysis;Software;Natural language processing;Complexity theory;Program Synthesis;Codex;GitHub Copilot;Empirical Evaluation},

doi={10.1145/3524842.3528470}}


## 10-A Large-Scale Comparison of Python Code


@inproceedings{10.1145/3524842.3528447,

author = {Grotov, Konstantin and Titov, Sergey and Sotnikov, Vladimir and Golubev, Yaroslav and Bryksin, Timofey},

title = {A large-scale comparison of Python code in Jupyter notebooks and scripts},

year = {2022},

isbn = {9781450393034},

publisher = {Association for Computing Machinery},

address = {New York, NY, USA},

url = {https://doi.org/10.1145/3524842.3528447},

doi = {10.1145/3524842.3528447},

abstract = {In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.},

booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},

pages = {353–364},

numpages = {12},

location = {Pittsburgh, Pennsylvania},

series = {MSR '22}

}


## 11-Tool Choice Matters JavaScript Quality Assurance


@INPROCEEDINGS{8812106,

author={Kavaler, David and Trockman, Asher and Vasilescu, Bogdan and Filkov, Vladimir},

booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},

title={Tool Choice Matters: JavaScript Quality Assurance Tools and Usage Outcomes in GitHub Projects},

year={2019},

volume={},

number={},

pages={476-487},

keywords={Tools;Task analysis;Pipelines;Quality assurance;Automation;Software;Switches;quality assurance tools;empirical study},

doi={10.1109/ICSE.2019.00060}}


## 12-Why Do People Give Up FLOSSing? A Study of Contributor Disengagement in Open Source

@InProceedings{10.1007/978-3-030-20883-7\_11,

author="Miller, Courtney

and Widder, David Gray

and K{\"a}stner, Christian

and Vasilescu, Bogdan",

editor="Bordeleau, Francis

and Sillitti, Alberto

and Meirelles, Paulo

and Lenarduzzi, Valentina",

title="Why Do People Give Up FLOSSing? A Study of Contributor Disengagement in Open Source",

booktitle="Open Source Systems",

year="2019",

publisher="Springer International Publishing",

address="Cham",

pages="116--129",

abstract="Established contributors are the backbone of many free/libre open source software (FLOSS) projects. Previous research has shown that it is critically important for projects to retain contributors and it has also revealed the motivations behind why contributors choose to participate in FLOSS in the first place. However, there has been limited research done on the reasons why established contributors disengage, and factors (on an individual and project level) that predict their disengagement. In this paper, we conduct a mixed-methods empirical study, combining surveys and survival modeling, to identify the reasons and predictive factors behind established contributor disengagement. We find that different groups of established contributors tend to disengage for different reasons; however, overall contributors most commonly cite some kind of transition (e.g., switching jobs or leaving academia). We also find that factors such as the popularity of the projects a contributor works on, whether they have experienced a transition, when they work, and how much they work are all factors that can be used to predict their disengagement from open source.",

isbn="978-3-030-20883-7"

}


## 13-Philanthropic conference-based requirements engineering in time of pandemic and beyond

Levy, M., Hadar, I., Horkoff, J. et al. Philanthropic conference-based requirements engineering in time of pandemic and beyond. Requirements Eng 28, 213–227 (2023). https://doi.org/10.1007/s00766-022-00386-4


## 14-The state of practice in requirements specification an extended

@article{10.1007/s00766-023-00399-7,

author = {Franch, Xavier and Palomares, Cristina and Quer, Carme and Chatzipetrou, Panagiota and Gorschek, Tony},

title = {The state-of-practice in requirements specification: an extended interview study at 12 companies},

year = {2023},

issue\_date = {Sep 2023},

publisher = {Springer-Verlag},

address = {Berlin, Heidelberg},

volume = {28},

number = {3},

issn = {0947-3602},

url = {https://doi.org/10.1007/s00766-023-00399-7},

doi = {10.1007/s00766-023-00399-7},

abstract = {Requirements specification is a core activity in the requirements engineering phase of a software development project. Researchers have contributed extensively to the field of requirements specification, but the extent to which their proposals have been adopted in practice remains unclear. We gathered evidence about the state of practice in requirements specification by focussing on the artefacts used in this activity, the application of templates or guidelines, how requirements are structured in the specification document, what tools practitioners use to specify requirements, and what challenges they face. We conducted an interview-based survey study involving 24 practitioners from 12 different Swedish IT companies. We recorded the interviews and analysed these recordings, primarily by using qualitative methods. Natural language constitutes the main specification artefact but is usually accompanied by some other type of instrument. Most requirements specifications use templates or guidelines, although they seldom follow any fixed standard. Requirements are always structured in the document according to the main functionalities of the system or to project areas or system parts. Different types of tools, including MS Office tools, are used, either individually or combined, in the compilation of requirements specifications. We also note that challenges related to the use of natural language (dealing with ambiguity, inconsistency, and incompleteness) are the most frequent challenges that practitioners face in the compilation of requirements specifications. These findings are contextualized in terms of demographic factors related to the individual interviewees, the organization they are affiliated with, and the project they selected to discuss during our interviews. A number of our findings have been previously reported in related studies. These findings show that, in spite of the large number of notations, models and tools proposed from academia for improving requirements specification, practitioners still mainly rely on plain natural language and general-purpose tool support. We expect more empirical studies in this area in order to better understand the reason of this low adoption of research results.},

journal = {Requir. Eng.},

month = {apr},

pages = {377–409},

numpages = {33},

keywords = {Requirements engineering, Requirements specification, Requirements documentation, Natural language requirements, Requirements management tools, Empirical studies, Interviews}

}


## 15-Why don’t we trace A study on the barriers to software traceability

Ruiz, M., Hu, J.Y. & Dalpiaz, F. Why don’t we trace? A study on the barriers to software traceability in practice. Requirements Eng 28, 619–637 (2023). https://doi.org/10.1007/s00766-023-00408-9





